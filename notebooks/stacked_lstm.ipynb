{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import hp, fmin, tpe, rand, Trials, STATUS_OK\n",
    "import optunity\n",
    "import ConfigSpace as CS\n",
    "from hpbandster.core.worker import Worker\n",
    "from hpbandster.optimizers import BOHB\n",
    "\n",
    "# Import helper modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../src\"))\n",
    "sys.path.append(project_root)\n",
    "from helpers import Utils\n",
    "from optimization.GA_runner import GARunner as GARunner\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"../models/stacked_lstm\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "DATASET_DIR = \"../data/processed\"\n",
    "\n",
    "# Define the model parameters\n",
    "CONFIG = {\n",
    "    'MAX_EVAL': 10,\n",
    "    'NBR_REP': 1\n",
    "}\n",
    "\n",
    "COLUMNS_RES = [\"proj\", \"algo\", \"iter\", \"AUC\", \"accuracy\", \"F1\", \"exp\"]\n",
    "MODEL_NAME = \"lstm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(dataset_train, time_step):\n",
    "    feature_cols = [col for col in dataset_train.columns \n",
    "                    if col not in ['build_failed', 'gh_build_started_at', 'gh_project_name'] \n",
    "                    and dataset_train[col].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
    "    training_set = dataset_train[feature_cols].values\n",
    "    y = dataset_train['build_failed'].values\n",
    "\n",
    "    # Limit time_step to the length of the training set\n",
    "    if len(training_set) < time_step:\n",
    "        print(f\"Adjusting time_step from {time_step} to {len(training_set) - 1}\")\n",
    "        time_step = max(1, len(training_set) - 1)\n",
    "\n",
    "    if Utils.CONFIG['WITH_SMOTE'] and len(np.unique(y)) > 1:\n",
    "        print(\"\\nClass Distribution BEFORE SMOTE:\")\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        dist = dict(zip(unique, counts / len(y)))\n",
    "        print(dist)\n",
    "\n",
    "    if Utils.CONFIG['WITH_SMOTE']:\n",
    "        print(\"\\nApplying SMOTE...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X, y_smote = smote.fit_resample(training_set, y)\n",
    "        training_set = X\n",
    "    else:\n",
    "        y_smote = y\n",
    "\n",
    "    if Utils.CONFIG['WITH_SMOTE'] and len(np.unique(y_smote)) > 1:\n",
    "        print(\"Class Distribution AFTER SMOTE:\")\n",
    "        unique, counts = np.unique(y_smote, return_counts=True)\n",
    "        dist = dict(zip(unique, counts / len(y_smote)))\n",
    "        print(dist)\n",
    "\n",
    "    try:\n",
    "        X_train = np.lib.stride_tricks.sliding_window_view(\n",
    "            training_set, (time_step, training_set.shape[1])\n",
    "        )[:-1]\n",
    "        X_train = np.squeeze(X_train, axis=1)\n",
    "        y_train = y_smote[time_step:]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during sliding window creation: {e}\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    return X_train, y_train\n",
    "\n",
    "def test_preprocess(dataset_train, dataset_test, time_step):\n",
    "    feature_cols = [col for col in dataset_train.columns \n",
    "                    if col not in ['build_failed', 'gh_build_started_at', 'gh_project_name'] \n",
    "                    and dataset_train[col].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
    "    train_data = dataset_train[feature_cols].values\n",
    "    test_data = dataset_test[feature_cols].values\n",
    "    dataset_total = np.vstack((train_data, test_data))\n",
    "    y_test = dataset_test['build_failed'].values\n",
    "    \n",
    "    if len(dataset_total) < time_step + len(dataset_test):\n",
    "        raise ValueError(\"Not enough data for test sequences\")\n",
    "    \n",
    "    inputs = dataset_total[-len(dataset_test) - time_step:]\n",
    "    X_test = np.lib.stride_tricks.sliding_window_view(inputs, (time_step, inputs.shape[1]))[:-1]\n",
    "    X_test = np.squeeze(X_test, axis=1)\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_lstm_model(network_params, train_set):\n",
    "    start_time = timer()\n",
    "    # Construct and train the LSTM model.\n",
    "    X_train, y_train = train_preprocess(train_set, network_params[\"time_step\"])\n",
    "    drop = network_params[\"drop_proba\"]\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with input shape\n",
    "    model.add(LSTM(units=network_params[\"nb_units\"],\n",
    "                   return_sequences=(network_params[\"nb_layers\"] > 1),\n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(drop))\n",
    "    \n",
    "    # Additional LSTM layers if specified\n",
    "    for i in range(1, network_params[\"nb_layers\"]):\n",
    "        is_last = (i == network_params[\"nb_layers\"] - 1)\n",
    "        model.add(LSTM(units=network_params[\"nb_units\"], return_sequences=not is_last))\n",
    "        model.add(Dropout(drop))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # print(f\"Model summary:\\n{model.summary()}\")\n",
    "    \n",
    "    model.compile(optimizer=network_params[\"optimizer\"], loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train)\n",
    "    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "    try:\n",
    "        history = model.fit(X_train, y_train, epochs=network_params[\"nb_epochs\"],\n",
    "                        batch_size=network_params[\"nb_batch\"], validation_split=0.2,\n",
    "                        verbose=0, callbacks=[es], class_weight=class_weight_dict)\n",
    "        validation_loss = np.amin(history.history['val_loss'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        return {\"validation_loss\": float('inf'), \"model\": None, \"entry\": {'F1': 0, 'validation_loss': float('inf')}}\n",
    "\n",
    "    entry = Utils.predict_lstm(model, X_train, y_train)\n",
    "    entry['validation_loss'] = validation_loss\n",
    "\n",
    "    end_time = timer()\n",
    "    print(f\"\\nTraining time: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # model_path = os.path.join(MODEL_DIR, f\"lstm_{network_params['nb_units']}_{network_params['nb_layers']}.keras\")\n",
    "    # model.save(model_path)\n",
    "    # print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    return {'validation_loss': validation_loss, 'model': model, 'entry': entry}\n",
    "\n",
    "def train_lstm_with_hyperopt(network_params):\n",
    "    # Train LSTM with hyperopt.\n",
    "    if 'data' not in globals():\n",
    "        raise ValueError(\"Global 'data' not set. Ensure evaluate_tuner sets it correctly.\")\n",
    "    res = construct_lstm_model(network_params, globals()['data'])\n",
    "    return {'loss': res['validation_loss'], 'status': STATUS_OK}\n",
    "\n",
    "def convert_from_PSO(network_params):\n",
    "    # Convert PSO parameters to appropriate types.\n",
    "    for key in network_params:\n",
    "        if key == 'optimizer':\n",
    "            network_params[key] = 'adam' if int(network_params[key]) == 1 else 'rmsprop'\n",
    "        elif key == 'nb_layers':\n",
    "            network_params[key] = int(network_params[key])\n",
    "    return network_params\n",
    "\n",
    "def fn_lstm_pso(drop_proba=0.01, nb_units=32, nb_epochs=2, nb_batch=4, nb_layers=1, optimizer=1, time_step=30):\n",
    "    # Function for PSO optimization.\n",
    "    optimizer = 'adam' if int(optimizer) == 1 else 'rmsprop'\n",
    "    network_params = {\n",
    "        'nb_units': int(nb_units),\n",
    "        'nb_layers': int(nb_layers),\n",
    "        'optimizer': optimizer,\n",
    "        'time_step': int(time_step),\n",
    "        'nb_epochs': int(nb_epochs),\n",
    "        'nb_batch': int(nb_batch),\n",
    "        'drop_proba': drop_proba\n",
    "    }\n",
    "    if 'data' not in globals():\n",
    "        raise ValueError(\"Global 'data' not set.\")\n",
    "    res = construct_lstm_model(network_params, globals()['data'])\n",
    "    return 1 - float(res[\"validation_loss\"])\n",
    "\n",
    "class LSTMWorker(Worker):\n",
    "    def __init__(self, train_set, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_set = train_set\n",
    "\n",
    "    def compute(self, config, budget, **kwargs):\n",
    "        res = construct_lstm_model(config, self.train_set)\n",
    "        return {'loss': res['validation_loss'], 'info': {}}\n",
    "\n",
    "def evaluate_tuner(tuner_option, train_set):\n",
    "    # Evaluate the specified tuner.\n",
    "    global data\n",
    "    data = train_set\n",
    "\n",
    "    # Define explicit parameter space for GA\n",
    "    all_possible_params = {\n",
    "        'drop_proba': list(np.linspace(0.01, 0.21, 20)),\n",
    "        'nb_units': [32, 64],\n",
    "        'nb_epochs': [4, 5, 6],\n",
    "        'nb_batch': [4, 8, 16, 32, 64], # Power of 2\n",
    "        'nb_layers': [1, 2, 3, 4],\n",
    "        'optimizer': ['adam', 'rmsprop'],\n",
    "        'time_step': list(range(30, 61))\n",
    "    }\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    if tuner_option == \"ga\":\n",
    "        ga_runner = GARunner()\n",
    "        best_params, best_model, entry_train = ga_runner.generate(all_possible_params, construct_lstm_model, data)\n",
    "\n",
    "    elif tuner_option == \"tpe\":\n",
    "        param_space = {k: hp.choice(k, v) for k, v in all_possible_params.items()}\n",
    "        trials = Trials()\n",
    "        best = fmin(train_lstm_with_hyperopt, param_space, algo=tpe.suggest, max_evals=CONFIG.get('MAX_EVAL'), trials=trials)\n",
    "        best_params = {k: all_possible_params[k][v] for k, v in best.items()}\n",
    "        res = construct_lstm_model(best_params, data)\n",
    "        entry_train, best_model = res[\"entry\"], res[\"model\"]\n",
    "\n",
    "    elif tuner_option == \"pso\":\n",
    "        params_PSO = {\n",
    "            'nb_units': [all_possible_params['nb_units'][0], all_possible_params['nb_units'][-1]],\n",
    "            'nb_layers': [all_possible_params['nb_layers'][0], all_possible_params['nb_layers'][-1]],\n",
    "            'optimizer': [1, 2],  # 1: adam, 2: rmsprop\n",
    "            'time_step': [all_possible_params['time_step'][0], all_possible_params['time_step'][-1]],\n",
    "            'nb_epochs': [all_possible_params['nb_epochs'][0], all_possible_params['nb_epochs'][-1]],\n",
    "            'nb_batch': [all_possible_params['nb_batch'][0], all_possible_params['nb_batch'][-1]],\n",
    "            'drop_proba': [all_possible_params['drop_proba'][0], all_possible_params['drop_proba'][-1]]\n",
    "        }\n",
    "        best_params, _, _ = optunity.maximize_structured(fn_lstm_pso, params_PSO, num_evals=CONFIG.get('MAX_EVAL'))\n",
    "        best_params = convert_from_PSO(best_params)\n",
    "        res = construct_lstm_model(best_params, data)\n",
    "        entry_train, best_model = res[\"entry\"], res[\"model\"]\n",
    "        \n",
    "    elif tuner_option == \"bohb\":\n",
    "        config_space = CS.ConfigurationSpace()\n",
    "        config_space.add(CS.UniformIntegerHyperparameter('nb_units', lower=32, upper=64))\n",
    "        config_space.add(CS.UniformIntegerHyperparameter('nb_layers', lower=1, upper=4))\n",
    "        config_space.add(CS.CategoricalHyperparameter('optimizer', choices=['adam', 'rmsprop']))\n",
    "        config_space.add(CS.UniformIntegerHyperparameter('time_step', lower=30, upper=60))\n",
    "        config_space.add(CS.UniformIntegerHyperparameter('nb_epochs', lower=4, upper=6))\n",
    "        config_space.add(CS.UniformIntegerHyperparameter('nb_batch', lower=4, upper=64))\n",
    "        config_space.add(CS.UniformFloatHyperparameter('drop_proba', lower=0.01, upper=0.2))\n",
    "\n",
    "        import hpbandster.core.nameserver as hpns\n",
    "        NS = hpns.NameServer(run_id=\"LSTM\", host='127.0.0.1', port=None)\n",
    "        NS.start()\n",
    "        w = LSTMWorker(train_set=data, nameserver='127.0.0.1', run_id=\"LSTM\")\n",
    "        w.run(background=True)\n",
    "        bohb = BOHB(configspace=config_space, run_id=\"LSTM\", nameserver='127.0.0.1', min_budget=1, max_budget=CONFIG.get('NBR_SOL'))\n",
    "        res = bohb.run(n_iterations=CONFIG.get('NBR_GEN'))\n",
    "        best = res.get_incumbent_id()\n",
    "        best_params = res.get_id2config_mapping()[best]['config']\n",
    "        res = construct_lstm_model(best_params, data)\n",
    "        entry_train, best_model = res[\"entry\"], res[\"model\"]\n",
    "        bohb.shutdown(shutdown_workers=True)\n",
    "        NS.shutdown()\n",
    "\n",
    "    elif tuner_option == \"rs\":\n",
    "        param_space = {k: hp.choice(k, v) for k, v in all_possible_params.items()}\n",
    "        trials = Trials()\n",
    "        best = fmin(train_lstm_with_hyperopt, param_space, algo=rand.suggest, max_evals=CONFIG.get('MAX_EVAL', trials=trials))\n",
    "        best_params = {k: all_possible_params[k][v] for k, v in best.items()}\n",
    "        res = construct_lstm_model(best_params, data)\n",
    "        entry_train, best_model = res[\"entry\"], res[\"model\"]\n",
    "\n",
    "    elif tuner_option == \"default\":\n",
    "        best_params = {\n",
    "            'nb_units': 64, 'nb_layers': 3, 'optimizer': 'adam', 'time_step': 30,\n",
    "            'nb_epochs': 10, 'nb_batch': 64, 'drop_proba': 0.1\n",
    "        }\n",
    "        res = construct_lstm_model(best_params, data)\n",
    "        entry_train, best_model = res[\"entry\"], res[\"model\"]\n",
    "\n",
    "    end = timer()\n",
    "    entry_train.update({\"time\": end - start, \"params\": best_params, \"model\": best_model})\n",
    "    # best_model_path = os.path.join(MODEL_DIR, f\"best_lstm_{proj_name}_fold{fold_idx}_iter{iter_idx}.keras\")\n",
    "    # best_model.save(best_model_path)\n",
    "    # print(f\"Best model saved at: {best_model_path}\")\n",
    "    return entry_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_entries, test_entries, title):\n",
    "    train_df = pd.DataFrame(train_entries)[COLUMNS_RES]\n",
    "    test_df = pd.DataFrame(test_entries)[COLUMNS_RES]\n",
    "    \n",
    "    print(f\"\\n{title} - Train Results:\")\n",
    "    print(train_df.groupby(['proj', 'exp']).mean())\n",
    "    print(f\"\\n{title} - Test Results:\")\n",
    "    print(test_df.groupby(['proj', 'exp']).mean())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    for i, metric in enumerate(['AUC', 'accuracy', 'F1']):\n",
    "        test_df.boxplot(column=metric, by='proj', ax=axes[i])\n",
    "        axes[i].set_title(f\"{metric} ({title})\")\n",
    "        axes[i].set_xlabel(\"Project\")\n",
    "        axes[i].set_ylabel(metric)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_online_validation(tuner=\"ga\", dataset_dir=DATASET_DIR):\n",
    "    # Run online validation and plot results.\n",
    "    all_train_entries = []\n",
    "    all_test_entries = []\n",
    "\n",
    "    print(f\"Loading datasets from {dataset_dir}...\")\n",
    "    dataset_sizes = {}\n",
    "    \n",
    "    # Get number of rows for each dataset\n",
    "    for f in os.listdir(dataset_dir):\n",
    "        try:\n",
    "            df = Utils.get_dataset(f, dataset_dir)\n",
    "            dataset_sizes[f] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    \n",
    "    # Sort and select top 10 largest datasets\n",
    "    top_10_files = sorted(dataset_sizes.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top_10_files = [f for f, _ in top_10_files]\n",
    "    \n",
    "    # Load only the top 10 datasets\n",
    "    datasets = {}\n",
    "    for f in top_10_files:\n",
    "        try:\n",
    "            datasets[f] = Utils.get_dataset(f, dataset_dir)\n",
    "            print(f\"Loaded {f} with {len(datasets[f])} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "    \n",
    "    print(f\"Selected top 10 datasets: {list(datasets.keys())}\")\n",
    "    if not datasets:\n",
    "        raise ValueError(f\"No datasets found in {dataset_dir}\")\n",
    "        \n",
    "    # for f in os.listdir(dataset_dir):\n",
    "    #     try:\n",
    "    #         datasets[f] = Utils.get_dataset(f, dataset_dir)\n",
    "    #         print(f\"Loaded {f} with {len(datasets[f])} samples\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error loading {f}: {e}\")\n",
    "    # print(f\"Loaded datasets: {list(datasets.keys())}\")\n",
    "    # if not datasets:\n",
    "    #     raise ValueError(f\"No datasets found in {dataset_dir}\")\n",
    "\n",
    "    for file_name, dataset in datasets.items():\n",
    "        best_f1 = -1\n",
    "        best_model_path = None\n",
    "        train_sets, test_sets = Utils.online_validation_folds(dataset)\n",
    "        for fold_idx, (train_set, test_set) in enumerate(zip(train_sets, test_sets)):\n",
    "            for iteration in range(1, CONFIG['NBR_REP'] + 1):\n",
    "                print(f\"\\n[Proj {file_name} | Fold {fold_idx+1} | Iter {iteration}] Training...\")\n",
    "                entry_train = evaluate_tuner(tuner, train_set)\n",
    "                entry_train.update({\n",
    "                    \"iter\": iteration, \"proj\": f\"proj{file_name}\", \"exp\": fold_idx + 1, \"algo\": MODEL_NAME\n",
    "                })\n",
    "                all_train_entries.append(entry_train)\n",
    "\n",
    "                best_model = entry_train[\"model\"]\n",
    "                best_params = entry_train[\"params\"]\n",
    "                X_test, y_test = test_preprocess(train_set, test_set, best_params[\"time_step\"])\n",
    "                entry_test = Utils.predict_lstm(best_model, X_test, y_test)\n",
    "                entry_test.update({\n",
    "                    \"iter\": iteration, \"proj\": file_name, \"exp\": fold_idx + 1, \"algo\": MODEL_NAME\n",
    "                })\n",
    "                if entry_test[\"F1\"] > best_f1:\n",
    "                    best_f1 = entry_test[\"F1\"]\n",
    "                    best_model_path = os.path.join(MODEL_DIR, f\"best_stacked_lstm_{file_name}.keras\")\n",
    "                    best_model.save(best_model_path)\n",
    "                print(f\"Test metrics: {entry_test}\")\n",
    "                all_test_entries.append(entry_test)\n",
    "        print(f\"Best model for {file_name} saved at: {best_model_path}, F1: {best_f1}\")\n",
    "\n",
    "    test_df = pd.DataFrame(all_test_entries)\n",
    "    proj_scores = test_df.groupby('proj')[['F1', 'AUC', 'accuracy']].mean()\n",
    "    print(\"\\nAverage Test Metrics by Project:\")\n",
    "    print(proj_scores)\n",
    "    bellwether = proj_scores['F1'].idxmax()\n",
    "    print(f\"\\nSelected Bellwether: {bellwether} (Best F1: {proj_scores.loc[bellwether, 'F1']:.4f})\")\n",
    "\n",
    "    # Plot the results\n",
    "    plot_metrics(all_train_entries, all_test_entries, \"Online Validation\")\n",
    "    return datasets[bellwether], datasets\n",
    "    \n",
    "def run_cross_project_validation(bellwether_dataset, all_datasets, tuner=\"ga\"):\n",
    "    # Run cross-project validation and plot results.\n",
    "    all_train_entries = []\n",
    "    all_test_entries = []\n",
    "\n",
    "    for iteration in range(1, CONFIG['NBR_REP'] + 1):\n",
    "        print(f\"[Cross-Project | Iter {iteration}] Training on Bellwether...\")\n",
    "        entry_train = evaluate_tuner(tuner, bellwether_dataset)\n",
    "        best_model = entry_train[\"model\"]\n",
    "        best_params = entry_train[\"params\"]\n",
    "        entry_train.update({\n",
    "            \"iter\": iteration, \"proj\": \"bellwether\", \"algo\": MODEL_NAME, \"exp\": 1\n",
    "        })\n",
    "        all_train_entries.append(entry_train)\n",
    "\n",
    "        for file_name, test_set in all_datasets.items():\n",
    "            if test_set is not bellwether_dataset:\n",
    "                best_f1 = -1\n",
    "                best_model_path = None\n",
    "                print(f\"Testing on {file_name}...\")\n",
    "                X_test, y_test = test_preprocess(bellwether_dataset, test_set, best_params[\"time_step\"])\n",
    "                entry_test = Utils.predict_lstm(best_model, X_test, y_test)\n",
    "                entry_test.update({\n",
    "                    \"iter\": iteration, \"proj\": file_name, \"exp\": 1, \"algo\": MODEL_NAME\n",
    "                })\n",
    "                \n",
    "                if entry_test[\"F1\"] > best_f1:\n",
    "                    best_f1 = entry_test[\"F1\"]\n",
    "                    best_model_path = os.path.join(MODEL_DIR, f\"best_stacked_lstm_{file_name}_cross_iter{iteration}.keras\")\n",
    "                    best_model.save(best_model_path)\n",
    "                    print(f\"Best model for {file_name} saved at: {best_model_path}, F1: {best_f1}\")\n",
    "                print(f\"Test metrics: {entry_test}\")\n",
    "                all_test_entries.append(entry_test)\n",
    "\n",
    "    plot_metrics(all_train_entries, all_test_entries, \"Cross-Project Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the online validation and select the bellwether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Online Validation and Selecting Bellwether...\")\n",
    "bellwether_dataset, all_datasets = run_online_validation(tuner=\"ga\", dataset_dir=DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cross-project validation with the selected bellwether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning Cross-Project Validation with Selected Bellwether...\")\n",
    "run_cross_project_validation(bellwether_dataset, all_datasets, tuner=\"ga\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-build",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
